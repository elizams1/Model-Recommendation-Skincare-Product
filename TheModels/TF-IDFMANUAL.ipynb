{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ['facial wash normal jerawat allantoin glycerin salicylic acid', 'kind facial wash facial wash normal minyak kering kombinasi normal kusam jerawat aqua cocamidopropyl betaine propylene glycol trehalose',\n",
    "          'deep water face light makeup gel cleanser facial wash normal minyak kering kombinasi normal glycerin green tea']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_wrd = []\n",
    "for i in range(0, len(corpus)):\n",
    "  words = corpus[i].split()\n",
    "  for w in words:\n",
    "    if w not in the_wrd:\n",
    "      the_wrd.append(w)\n",
    "\n",
    "the_wrd = sorted(the_wrd)\n",
    "print(the_wrd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "n_data = len(corpus)\n",
    "n_kata = len(the_wrd)\n",
    "\n",
    "data_tf = pd.DataFrame(np.zeros((n_data, n_kata)), columns=list(the_wrd))\n",
    "\n",
    "# Compute Term Frequency (TF)\n",
    "temp = 0\n",
    "a = []\n",
    "for i in range(n_data):\n",
    "\n",
    "    words = corpus[i].split()  # Words in the document\n",
    "    print(words)\n",
    "    for w in words:\n",
    "        data_tf[w][i] = data_tf[w][i] + 1 / len(words)\n",
    "\n",
    "data_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Menjadikan dataset file xlsx\n",
    "data_tf.to_excel(\"C:/Users/HP/Downloads/TUGAS AKHIR/EXCEL KODINGAN/\" +'TF_Manual_Laporan.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = {}\n",
    "\n",
    "for w in the_wrd:\n",
    "    n = 0\n",
    "\n",
    "    for i in range(n_data):\n",
    "        if w in corpus[i].split():\n",
    "            n += 1\n",
    "    # a = 1+n_data\n",
    "    # b = 1+n\n",
    "    # print(a,b)\n",
    "    idf[w] = (np.log((1+n_data)/(1+n)))+1\n",
    "\n",
    "print(idf)\n",
    "data_idf = pd.DataFrame.from_dict(idf, orient='index', columns=['Nilai IDF'])\n",
    "\n",
    "data_idf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Menjadikan dataset file xlsx\n",
    "data_idf.to_excel(\"C:/Users/HP/Downloads/TUGAS AKHIR/EXCEL KODINGAN/\" +'IDF_Manual_Laporan.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tf_idf = data_tf.copy()\n",
    "\n",
    "for w in the_wrd:\n",
    "    for i in range(n_data):\n",
    "        data_tf_idf[w][i] = data_tf[w][i] * idf[w]\n",
    "\n",
    "data_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Menjadikan dataset file xlsx\n",
    "data_tf_idf.to_excel(\"C:/Users/HP/Downloads/TUGAS AKHIR/EXCEL KODINGAN/\" +'TF_IDF_Manual_Laporan.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melakukan normalisasi\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "\n",
    "normalizd_arr = preprocessing.normalize(data_tf_idf)\n",
    "\n",
    "after_normz = pd.DataFrame(normalizd_arr, columns=list(the_wrd))\n",
    "after_normz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Menjadikan dataset file xlsx\n",
    "after_normz.to_excel(\"C:/Users/HP/Downloads/TUGAS AKHIR/EXCEL KODINGAN/\" +'TF_IDF_Manual(AfterNormz)_Laporan.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(analyzer='word')\n",
    "tfidf_dataset_coba = vectorizer.fit_transform(corpus)\n",
    "\n",
    "df_tfidf_sklearn = pd.DataFrame(\n",
    "    tfidf_dataset_coba.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "tfidf_tokens = vectorizer.get_feature_names_out()\n",
    "df_tfidfvect = pd.DataFrame(data=tfidf_dataset_coba.toarray(\n",
    "), index=[0, 1, 2], columns=tfidf_tokens)\n",
    "\n",
    "df_tfidfvect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cos_sim = cosine_similarity(data_tf_idf, data_tf_idf)\n",
    "# print(cos_sim)\n",
    "df_cosim = pd.DataFrame(data=cos_sim)\n",
    "display(df_cosim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cos_sim = cosine_similarity(tfidf_dataset_coba, tfidf_dataset_coba)\n",
    "# print(cos_sim)\n",
    "df_cosim = pd.DataFrame(data=cos_sim)\n",
    "display(df_cosim)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
